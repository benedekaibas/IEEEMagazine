\documentclass{IEEEcsmag}

\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}
\expandafter\def\expandafter\UrlBreaks\expandafter{\UrlBreaks\do\/\do\*\do\-\do\~\do\'\do\"\do\-}
\usepackage{upmath,color}


\jvol{XX}
\jnum{XX}
\paper{8}
\jmonth{Month}
\jname{Publication Name}
\jtitle{Publication Title}
\pubyear{2021}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}


\setcounter{secnumdepth}{0}

\begin{document}

\sptitle{Article Type: Description  (see below for more detail)}

\title{Title: An Empirical Study on Static Analyzer Toolsets to Reduce False Positives, False Negatives in Python Type Checkers}

\author{Benedek Kaibas}
\affil{Allegheny College, PA, 16335, USA}

\author{Dr. Gregory M. Kapfhammer}
\affil{Allegheny College, PA, 16335, USA}

\markboth{THEME/FEATURE/DEPARTMENT}{THEME/FEATURE/DEPARTMENT}

\begin{abstract}\looseness-1 Pythonâ€™s dominance in large-scale systems necessitates robust static type checkers like Mypy and Pyright. But as the language evolves, who verifies these tools? We introduce Pytifex, a framework that bridges the gap between theoretical fuzzing and real-world defects. Unlike random code generators, Pytifex mines and mutates closed GitHub issues to create targeted, adversarial test cases. This column details how data-driven differential testing reveals significant soundness gaps in major type checkers. We offer a roadmap for tool maintainers to use these refined examples to fortify the Python ecosystem against false diagnostics. 
\end{abstract}

\maketitle
